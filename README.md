<p align="center">
  <img src="img/capa projeto.png" />
</p>

# AWS S3 Medallion ETL Pipeline üèÖ

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![AWS](https://img.shields.io/badge/AWS-S3-orange.svg)
![Poetry](https://img.shields.io/badge/packaging-poetry-cyan.svg)
![Status](https://img.shields.io/badge/status-completed-green.svg)

Este projeto implementa um pipeline de Engenharia de Dados completo (ETL), movendo dados de um ambiente local para a nuvem AWS S3. O projeto segue a **Arquitetura Medalh√£o** (Bronze, Silver e Gold) para garantir qualidade, rastreabilidade e organiza√ß√£o dos dados.

## üèó Arquitetura do Projeto

Abaixo, o fluxo de dados implementado:


<p align="center">
  <img src="img/diagrama1.png" />
</p>

O fluxo de dados segue as seguintes etapas:

1.  **Ingest√£o (Local ‚Üí Bronze):**
    - Upload de arquivos CSV brutos (Raw Data) para o bucket S3 na pasta `raw/`.
    - Garante backup e imutabilidade dos dados originais.

2.  **Processamento (Bronze ‚Üí Silver):**
    - Leitura dos dados brutos.
    - Limpeza e padroniza√ß√£o utilizando **Pandas**.
    - Adi√ß√£o de metadados (ex: data de ingest√£o).
    - Convers√£o para formato **Parquet** (colunar e comprimido) e upload para `silver/`.

3.  **Agrega√ß√£o (Silver ‚Üí Gold):**
    - Leitura dos dados refinados (Parquet).
    - Aplica√ß√£o de regras de neg√≥cio e agrega√ß√µes.
    - Cria√ß√£o de uma vis√£o anal√≠tica pronta para consumo e upload para `gold/`.

4.  **Verifica√ß√£o:**
    - Valida√ß√£o dos dados diretamente na nuvem utilizando leitura via `s3fs` sem necessidade de download f√≠sico.

```
aws-s3-etl-medallion/
‚îú‚îÄ‚îÄ data/                  # Pasta local para entrada de arquivos CSV
‚îú‚îÄ‚îÄ main.py                # Script principal do pipeline ETL
‚îú‚îÄ‚îÄ pyproject.toml         # Gerenciamento de depend√™ncias (Poetry)
‚îú‚îÄ‚îÄ poetry.lock            # Vers√µes travadas das bibliotecas
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o do projeto
‚îî‚îÄ‚îÄ .env                   # Vari√°veis de ambiente (n√£o versionado)
```

## üõ† Tecnologias Utilizadas

- **Linguagem:** Python 3.11.12
- **Cloud:** AWS S3 (Simple Storage Service)
- **Manipula√ß√£o de Dados:** Pandas
- **Formatos de Arquivo:** CSV, Parquet
- **Bibliotecas:** `boto3`, `s3fs`, `python-dotenv`, `pandas`

## üöÄ Como Executar
**Pr√©-requisitos**
- Python 3.11+ instalado.
- Poetry instalado.
- Conta na AWS com permiss√£o de escrita no S3.

###  Passo a Passo
1. Clone o reposit√≥rio:

```
git clone [https://github.com/diipdata/aws-s3-etl-medallion.git](https://github.com/diipdata/aws-s3-etl-medallion.git)
cd aws-s3-etl-medallion
```

2. Instale as depend√™ncias:
```
poetry install
```

3. Configure as credenciais: Crie um arquivo .env na raiz do projeto seguindo o exemplo:

```
AWS_ACCESS_KEY_ID=sua_chave
AWS_SECRET_ACCESS_KEY=sua_senha
AWS_REGION=us-east-1
BUCKET_NAME=nome-do-seu-bucket
```

4. Adicione dados: Coloque seus arquivos CSV na pasta *data/* na raiz do projeto.

5. Execute o pipeline:

```
poetry run python main.py
```

## Pr√≥ximos Passos (Roadmap)
Este projeto serve como funda√ß√£o para um Data Lake. As pr√≥ximas evolu√ß√µes planejadas s√£o:

[ ] Migra√ß√£o para Cloud Native: Mover a execu√ß√£o do script local para uma fun√ß√£o AWS Lambda, automatizando a execu√ß√£o sempre que um arquivo cair no bucket.

[ ] Data Quality: Implementar valida√ß√µes robustas (ex: Great Expectations ou Pandera) para garantir que dados corrompidos n√£o cheguem √† camada Silver.

[ ] Cataloga√ß√£o: Utilizar o AWS Glue Crawler para catalogar as tabelas Gold e permitir consultas via SQL (AWS Athena).

[ ] IaC (Infrastructure as Code): Criar os buckets e permiss√µes automaticamente utilizando Terraform.

[ ] Dashboard: Conectar o Power BI ou QuickSight na camada Gold para visualiza√ß√£o dos dados.

----


**Criado por [Diego](https://github.com/diipdata)**  
diegop.freitas@gmail.com | [LinkedIn](https://linkedin.com/in/diegop-freitas) | [X/Twitter](https://x.com/diipdata)

*Feito com ‚òï e muitas linhas de c√≥digo (e alguns erros pelo caminho).*