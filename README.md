# AWS S3 Medallion ETL Pipeline üèÖ

Este projeto implementa um pipeline de Engenharia de Dados completo (ETL), movendo dados de um ambiente local para a nuvem AWS S3, seguindo a **Arquitetura Medalh√£o** (Bronze, Silver e Gold).

## üèó Arquitetura do Projeto

O fluxo de dados segue as seguintes etapas:

1.  **Ingest√£o (Local ‚Üí Bronze):**
    - Upload de arquivos CSV brutos (Raw Data) para o bucket S3 na pasta `raw/`.
    - Garante backup e imutabilidade dos dados originais.

2.  **Processamento (Bronze ‚Üí Silver):**
    - Leitura dos dados brutos.
    - Limpeza e padroniza√ß√£o utilizando **Pandas**.
    - Adi√ß√£o de metadados (ex: data de ingest√£o).
    - Convers√£o para formato **Parquet** (colunar e comprimido) e upload para `silver/`.

3.  **Agrega√ß√£o (Silver ‚Üí Gold):**
    - Leitura dos dados refinados (Parquet).
    - Aplica√ß√£o de regras de neg√≥cio e agrega√ß√µes.
    - Cria√ß√£o de uma vis√£o anal√≠tica pronta para consumo e upload para `gold/`.

4.  **Verifica√ß√£o:**
    - Valida√ß√£o dos dados diretamente na nuvem utilizando leitura via `s3fs` sem necessidade de download f√≠sico.

## üõ† Tecnologias Utilizadas

- **Linguagem:** Python 3.11.12
- **Cloud:** AWS S3 (Simple Storage Service)
- **Manipula√ß√£o de Dados:** Pandas
- **Formatos de Arquivo:** CSV, Parquet
- **Bibliotecas:** `boto3`, `s3fs`, `python-dotenv`, `pandas`

## üöÄ Como Executar

1. Clone o reposit√≥rio.
2. Crie um arquivo `.env` com suas credenciais AWS:
   ```env
   AWS_ACCESS_KEY_ID=sua_chave
   AWS_SECRET_ACCESS_KEY=sua_senha
   AWS_REGION=us-east-1
   BUCKET_NAME=nome-do-seu-bucket
3. Coloque seus arquivos CSV na pasta downloads/.
4. Execute o pipeline:

    ``python main.py``

----


**Criado por [Diego](https://github.com/diipdata)**  
diegop.freitas@gmail.com | [LinkedIn](https://linkedin.com/in/diegop-freitas) | [X/Twitter](https://x.com/diipdata)

*Feito com ‚òï e muitas linhas de c√≥digo (e alguns erros pelo caminho).*